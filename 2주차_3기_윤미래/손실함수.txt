손실함수:
	비용 함수, Loss Function
	예측값(선형)과 실제값(점) 사이 거리(오차)가 좁을수록 정확도가 높다.
	격차가 양의 값인지 음의 값인지가 아닌, 절대값으로 측정한다.
	이 오차를 측정하는 것이 손실함수.
MSE = (1/N) ∑^N_(i=1) (y_i - (w^Tx_i + b))^2
	N: 데이터 개수
	x_i: 입력값
	y_i: 실제값
	w^Tx_i + b: 예측값
		(예측값-실제값)^2 을 모두 더한 후, 평균을 구한다.
		↑ 제곱하는 이유: 음수값 위험 제거, 패널티 극대화
→ 실제적으로 기울기 a와 절편 b에 대한 함수.
(선형함수의 목적은 a와 b의 최소값을 구해 Loss(Cost)를 0에 수렴하는 것)

----------

더 찾아본 손실함수:
 Mean Squared Error
	MSE = (1/N) ∑^N_(i=1) (y_i - y^_i)^2
	공식이 간단. 차가 커질수록 제곱 연산으로 값이 뚜렷해진다.
 Root MSE
	RMSE = √∑^N_(i=1) (y^_ - y_i)^2 / n
	MSE에 루트를 씌운 형태. 값의 왜곡을 줄임.

# 여기부터 뭔소린지모르겠지만 일단적는중
 Binary Crossentropy
	BCE(x) = -(1/N) ∑^N_(i=1) y_i log(h(x_i;0)) + (1 - y_i) log (1 - h(x_i;0))
	실제 레이블과 예측 레이블 간의 교차 엔트로피 손실을 계산.레이블 클래스 2개일 때 추천.
	예측값=1, 실제값=1 → 0에 수렴
	예측값=0, 실제값=1 → ∞
 Categorical Crossentropy (Softmax loss)
	L = -(1/N) ∑^N_(j=1) ∑^C_(i=1) t_ij log(y_ij)
	레이블 클래스 3개 이상일 때 사용. 출력값을 클래스 소속 확률에 대한 예측으로 이해하는 문제에서 사용
	라벨 (0,0,1,0,0) , (0,1,0,0,0) ... (one-hot encoding) 형태로 제공될 때 사용
 Sparse categorical crossentropy
	멀티 클래스 분류에 사용. one-hot encoding 상태일 필요 없음. 정수 인코딩 상태에서 수행 가능. 라벨 (1,2,3,4)
	어... 중단합니다......